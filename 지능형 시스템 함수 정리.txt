데이터 적재(로딩)->데이터의 관찰(탐색)EDA->(문자열 데이터 전처리)->데이터의 분할->데이터 전처리->머신러닝 모델의 구축
데이터 전처리를 나중에 하는 이유는?		3주차 2시 8분쯤. 0과 1사이로 압축시켜놓고 데이터를 처리하면 안됨. 블랙프라이데이 현상은 설명 못하는것과 같은 원리. 
					학습의 성능은 올라갈 수 있으나 실전에서 망함. 답지를 먼저 봤냐 안봤냐.
					테스트까지 전부 포함해서 전처리를하면 정답도 알고있는거니깐 답을 알고 시험을 본거임.
					모르는 문제는 못함...
					전체 모습을 미리 확인한 후 데이터가 분할되는 결과를 가지기 때문에 학습의 성능은 올라가지만 실전에서 결과가 좋지 않은 모델이 생성될 수 있음
					전처리과정의 데이터 학습은 학습데이터를 기준으로 수행
앙상블을 쓰는 이유는?			일반화 시키기 위해서
컬럼 지우는 이유?				결측데이터가 많이 빠지거나, 고유한 값의 개수가 가장 많은것. 타이타닉 PassengerId, 이름, 티켓, cabin 데이터는 빼야되는 이유와 같음.
pandas의 inplace?				drop 등등 데이터프레임을 변경 할 때, True이면 원본 데이터에도 반영함, False이면 원본 데이터는 유지됨
모든 사이킷런의 전처리 클래스들은 transform 메소드의 결과가 numpy 배열로 반환 (pandas의 데이터프레임이 아님!!!) 즉, 다시 pandas dataFrame으로 바꿔줘야함.
**중요** RandomForestClassifier에서 max_depth=None이면 학습 결과는 100%이다.
제약을 쓰는 이유는?			일반화 성능(테스트 데이터의 성능)을 높여주기 위하여
문자열 데이터 전처리 이유는?		머신러닝 알고리즘은 문자열 처리 못해서. 문자열 데이터는 데이터 분할 전단계에서 함


df.info()					data type 확인(int,float 등등)
df.describe()				min,25%,50%,75%,max 확인. 숫자 정보만 나옴(문자 정보는 출력 안함)
df.describe(include='all')			전부 보여주기
df.describe(include='object')			문자열만 보여주기. count 결측데이터가 아닌 개수. unique 중복데이터가 아닌 개수. top 가장 많이 나온값은?, freq 그 값이 뭔지
print(X_train.shape,X_test.shape) 혹은 print(len(y_train),len(y_test)) 행열 개수 체크하는 방법. 대신 앞에껀 차원 전부 보여주고, 뒤에껀 한 차원만 보여줌. 
print(X.isnull())				결측 데이터 개수 확인
print(X.isnull().sum())			결측 데이터 개수 확인(캘리포니아 거주 데이터에서 합계를 통해 0이 아니면 다른방법 써야되는거임)
print(y.value_counts())			값의 개수 확인(분류형에서. 회귀형은 생략)
print(X_train.shape, X_test.shape)		데이터의 차원 정보를 반환
print(model.coef_)				선형 모델에서 특성의 가중치 확인하는 방법
model.fit(X_train,y_train)
model.score(X_train,y_train)
model.predict(X_test,y_test)
model.predict_proba(X_train[:5])		[예측값,예측값] 두예측값중에 큰것이 결과임
model.decision_function(X_train[-10:])		predict_proba와 비슷하게 -면 0, +면 1로 예측
pd.options.display.max_columns = 100		......표시 된거 없애고 모든 데이터 보여주기
pd.options.display.max_rows = 10		......표시 된거 없애고 모든 데이터 보여주기
X = data3.iloc[:, 1:]				[모든 컬럼 다가져오기, 두번째 열부터 데이터 가져오기]
y = data3.Survived				Survied 데이터 가져오기
X = pd.concat([X_num, X_obj_encoded], axis = 1)	concat 메소드를 사용하여 데이터프레임을 결합. axis=1은 좌우로 결합한다는 뜻. (바로쓰면 안됨. dropna 때매. 빠진 데이터까지 추가가 됨)
X_num.reset_index(inplace=True)		(바로 위에꺼 해결)index를 초기화 해서 맞춤
data2 = data.drop(columns=['PassengerId','Name','Ticket','Cabin'],inplace=False)	열 제거, inplace는 데이터에 반영할지 여부를 결정(True면 반영)
data3 = data2.dropna(subset=['Age','Embarked'])	행 제거

파이썬에서 리스트 빠르게 가져오는 방법
# 수치형 데이터의 컬럼명
X_num = [cname for cname in X.columns 		X.columns에 있는걸 하나하나씩 가져와서 cname에 넣기. 그걸 그대로 cname 리스트에 넣음
         if X[cname].dtype in ['int64','float64'] ]		int64, float64인거 가져오기
# 문자형 데이터의 컬럼명
X_obj = [cname for cname in X.columns 
         if X[cname].dtype not in ['int64','float64'] ]


설명변수 X				다차원. [column,row],  특정 종속변수를 유추하기 위해 정의된 데이터셋
종속변수 y				1차원  정답 데이터, label

분류형(범주형)				정확도를 반환.확률 값으로 예측 가능. 값의 개수를 확인하는게 좋음(예를 들면 0,1). 한가지에만 너무 치중되어있으면 99%가 맞다고 하니깐.
회귀형(선형)				결정계수를 반환. 값의 개수를 확인 안해도 됨(어차피 중복은 거의 없으므로)
					목표치는 0.7혹은 0.8 이상임.
					결정계수 0이면->머신러닝 모델의 학습이 부족함 (과소적합)
					결정계수 1이면->머신러닝 모델의 학습이 잘 진행됨을 확인 (과대적합의 의심...)
					결정계수 음수이면->머신러닝 모델의 학습이 부족함 (과소적합)
					회귀 분석의 경우 해당 특성의 값은 종속변수의 값을 증가시키기 위해서 중요도가 높음
					학습한 가중치는 학습 데이터에 베스트 핏이 되는 가중치(학습데이터의 베스트핏)
LinearRegression				학습 데이터에 최적화되도록 학습을 해서 테스트 데이터에 대한 일반화 성능이 감소.
					이를 해결하기 위해 제약조건을 통해 일반화 성능 증가시킴.
bootstraping				뽑기에서 1번이 나오면 또 1번이 나올 수 있다.
*Ridge(L2)				0주변으로 보내버림 but 0은 아님(궁금하면 그림참고). 모든 특성에 대해 제약 적용.
*Lasso(L1)				가중치가 0으로 수렴. 특정 특성에 대해 제약 적용.
라벨인코딩				공부하는 x데이터만 문자열만 아니면 된다. 일반적으로 정답 데이터(y)가 문자열로 구성되 있는 경우 사용
					반면 라벨 인코딩은 설명변수 X에는 잘 사용 안함(값에 대소가 생겨 가중치가 달라지므로). 
원핫인코딩				일반적으로 설명 변수(x)에 포함된 문자열 데이터의 전처리에 활용됨.
					메모리의 낭비가 심하므로 유일한 값의 케이스를 줄일 수 있는 방향을 접근하는 것이 유의미하다.


머신러닝에서 학습데이터 : 테스트데이터=(70% or 80%) : (30% or 20%)
딥러닝에서 학습데이터 : 검증 데이터 : 테스트데이터=(70%) : (10%) : (20%)
	딥러닝에서 검증데이터가 필요한 이유 : 부분배치 학습을 수행하여 점진적으로 학습량을 늘려가는 경우가 많아, 중간 점검의 의미로 검증데이터 사용.

from sklearn.model_selection import train_test_split		이건 그냥 숙지
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    stratify=y,		test,train의 y데이터 비율을 맞춤.
                                                    random_state=30)	매번 똑같이 뽑아주기위해


f:함수, p:하이퍼 파라미터
from sklearn.preprocessing import MinMaxScaler	전처리 0~1값으로 만들어주는것-->(원본값-최소값)/(최대값-최소값)
-f : scaler.fit(X_train)				**중요** 학습데이터만 해줘야함. 테스트 데이터는 학습데이터를 기준으로 바로 아래의 변환과정만 수행
-f : X_train=scaler.transform(X_train)			설명변수 전처리 해주는것. 
-f : pred=model.predict(X_train[:2])			2번째 X데이터를 가져와서 예측해보는 과정중 씀...
from sklearn.preprocessing import StandardScaler	정규화로 스케일 처리. 데이터를 구성하는 각 컬럼의 값을 평균은 0, 표준편차는 1로 스케일 조정
from sklearn.neighbors import KNeighborsClassifier	기하적 계산(유클리드). 분류형
-p : n_neighbors=3				3개 데이터를 기준으로...
-f : model.fit(X_train,y_train)
-f : score=model.score(X_train,y_train)
    score=model.score(X_test,y_test)
from sklearn.neighbors import KNeighborsRegressor	기하적 계산(유클리드). 회귀형
from sklearn.linear_model import LinearRegression	선형방정식을 기반으로 회귀분석 수행
						각각의 특성 별로 최적화된 가중치의 값을 계산하는 머신러닝 알고리즘.
						학습데이터에 베스트 핏이 되는 가중치->새로운 데이터에 잘 적응할 수 있을까->제약의 방식을 사용하여 문제해결(Ridge,Lasso) 
-f : coef_						기울기, 즉 가중치임(기울기가 0이면 결과에 영향 없는 특성임. 기울기의 절대값이 클수록 영향력 큼)
-f : intercept					절편, 즉 y를 예측하기 위한 절편의 값
from sklearn.linear_model import Ridge, Lasso		Ridge=L1, Lasso=L2
-p : alpha					클수록 제약 커짐(모델 학습을 방해), 알파값이 작아질수록 LinearRegression 클래스와 동일해짐
-p : random_state					
-p : max_iter=1000					반복횟수
from sklearn.metrics import R2_score				데이터에 관계없이 동일한 결과의 범위를 사용하여 모델을 평가
from sklearn.metrics import 	mean_absolute_error		실제 정답과 모델이 예측한 값의 차이를 절대값으로 평균(머신러닝 모델이 예측한 값의 신뢰 범위). 평가를 위해서는 머신러닝 모델이 예측한 값이 필요함
from sklearn.metrics import 	mean_absolute_percentage_error	실제 정답과 모델이 예측한 값의 비율 차이를 절대값으로 평균. 평가를 위해서는 머신러닝 모델이 예측한 값이 필요함
from sklearn.metrics import 	mean_squared_error		실제 정답과 모델이 예측한 값의 차이의 제곱값 평균(머신러닝/딥러닝 모델의 오차 값을 계산할 때 사용). 평가를 위해서는 머신러닝 모델이 예측한 값이 필요함 (공통)
from sklearn.preprocessing import PolynomialFeatures	
-p : degree=3					차원 조절
-p : include_bias=False				True(기본값)이면 모든 다항식 거듭제곱이 0인 특성(즉, 1의 열 - 선형 모델에서 절편 항으로 작동)인 편향 열을 포함합니다.
-f : transform(X)					차원 변경함수
from sklearn.linear_model import LogisticRegression	#####Regression이라 쓰여있는데 이거 분류형임#####
-p : penalty='l2',					탐색DB(Ridge,Lasso 등등)
-p : C=1.0					L1,L2 제약(알파값과 반대로 움직임)
-p : class_weight='balanced'				가중치 밸런스 맞추기(나는 1개말고, 99개 맞출거다. 객관식 90 서술형 10이면 객관식 준비하는것과 같은 원리)
-p : class_weight={0:1000, 1:1}			//class_weight 또다른 예시
-p : solver='lbfgs'					최적의 알고리즘을 적용하기 위해 있음. 데이터셋이 대용량이면 sag,saga로 하면댐
-p : max_iter=1000000				반복 횟수
-p : n_jobs=-1					
-p : random_state=5				
-p : verbose=3					진행과정 확인
from sklearn.metrics import confusion_matrix		혼동행렬
-f : confusion_matrix(y_train, pred)			
from sklearn.metrics import accuracy_score		정확도
from sklearn.metrics import precision_score		정밀도
-p : pos_label=0					0에 대한 정밀도
from sklearn.metrics import recall_score		재현율
-p : pos_label=0					0에 대한 재현율
from sklearn.tree import DecisionTreeClassifier		결정트리
-p : max_depth=3					과대적합이 defalut 값임
-p : class_weight='balanced'				
-p : random_state=1
-p : min_samples_split				최소 샘플의 개수. 늘어날수록 상세하게 분류 못함
-p : min_samples_leaf				deafault=1로 제일 마지막 leaf는 1개면 된다. 늘어날수록 상세하게 분류 못함
from sklearn.svm import SVC, LinearSVC		support vector임. support vector들을 찾는것이 목적이라 스케일 값에 굉장히 민감
						따라서 데이터 전처리가 필수다.
-p : kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’	linear이면 LinearSVC랑 똑같음. 가중치는 Linear인 경우에만 확인 가능
-p : gamma					밀집데이터의 강도를 결정
from sklearn.ensemble import VotingClassifier		앙상블 취합(여러개 모델)
-p : estimators=estimators				내가 사용할 모델. 오른쪽 estimators는 list형 자료형임. estimators = [('knn', m1), ('lr', m2), ('dt', m3)]
-p : voting='hard'					개수만 가지고 단하게 countig, 'soft'이면 비율임
from sklearn.ensemble import BaggingClassifier			앙상블 취합(한개 모델, 각각 모델이 학습하는 데이터는 무작위 추출 방법(부스트트래핑)으로 처리.)
							즉 다수개의 모델이 서로다른 관점의 학습을 진행 할 수 있도록 처리. "모델을 바꿀 수 없으니 데이터를 바꾸자"
-p : base_estimator=DecisionTreeClassifier(random_state=1)	모델 넣기	,전처리와 스케일이 필요 없음.
-p : n_estimator=10					n개의 base_estimator가 생성됨
-p : max_samples						샘플 개수(낮을 수록 공부 방해)
-p : max_features						특성(낮을 수록 공부 방해)
from sklearn.ensemble import RandomForestClassifier	배깅 앙상블에 결정트리를 조합한 모델
-p : n_estimators=100
-p : max_depth=None
-p : max_samples=0.7
-p : max_features=0.7
-p : n_jobs=-1
-p : random_state=1
from sklearn.ensemble import AdaBoostClassifier	앙상블 부스팅. 선형모델일때 무조건 써야댐. 기본모델을 직접 설정해야함
						데이터 중심의 부스팅 방법론 구현. 작전 모델이 잘못 예측한 데이터에 가중치를 부여
						"100개중 5개만 1차로 맞추면 다음모델은 남은 95개에 집중"
-p : learning_rate=1.9				얼마나 가중치를 둘것인지. 높을수록 더 많은 가중치 부여
from sklearn.ensemble import GradientBoostingClassifier	앙상블 부스팅. 결정 트리가 기본모델. "랜덤포레스트의 부스팅버전"
						오차에 중심을 둔 부스팅 방법론을 구현. 각각의 학습 데이터에 대해서 오차의 크기가 큰 데이터에 가중치를
						부여하여 전체의 오차를 줄여나가는 방식.
						"날씨 예측은 20도였는데 실제론 30도. 10도 차이를 줄이기 위한 노력을 한다"
-p : n_estimators=100				
-p : learning_rate=0.1				
-p : subsample=0.2				
-p : max_depth=1					
-p : random_state=1					
from sklearn.preprocessing import OneHotEncoder	원핫인코더의 주요 파라메터
-p : categories='auto'				내가 갖고 있는 데이터를 알아서 unique값을 뽑아서 쓴다.
-p : sparse=False,					희소행렬을 제어. 희소행렬을 쓸것인가?---> (몇번째자리)가 1이야~
-p : handle_unknown='ignore'			**중요** 자연어 처리할때 많이 발생. 채팅 메세지로 욕하는지 판단
						신조어를 인식하기 위함. 모르는 x가 들어오면 error가 발생함.
						unique한 문자를 어떻게 할지.. ignore를 하게 되면 다 0으로 만들어버림.
						학습 과정에서 인지하지 못한 문자열 값에 대한 처리 프로세스 정의
-f : fit						전처리 과정에 필요한 정보를 수집
-f : transform					fit 메소드에서 인지한 결과를 바탕으로 데이터를 변환하여 반환하는 메소드
-f : X_obj_encoded = encoder.fit_transform(X_obj)	위에 두 함수 합친거



그냥 숙지
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

X.info()
X.isnull().sum()
X.describe(include='all')

y.head()
y.value_counts()
y.value_counts() / len(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=50)

print(f'Train={score}')